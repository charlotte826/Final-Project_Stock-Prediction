{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import emoji\n",
    "import tweepy\n",
    "import csv\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import datetime, time\n",
    "import pickle\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMER_KEY = \"gascQ6nxzGIPHAJKVjIVVf3QE\"\n",
    "CONSUMER_SECRET = \"4ixOxsZ4rS6yWhlotLNVib465CdmavTUbIvVUCRHlajgdNlfD0\"\n",
    "ACCESS_TOKEN = \"858881393727877120-EqrBy6vqwxazAedJw92aTJnRmpnqUr6\"\n",
    "ACCESS_TOKEN_SECRET = \"WHYhY1Xd17YUrzOdmx3LUwqzQyz1XOVa5HFPx5ASSglRa\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screen name BobPisani\n",
      "getting tweets before 1004769626633277439\n",
      "getting tweets before 917359531243593727\n",
      "getting tweets before 856655540021276671\n",
      "getting tweets before 818942015324831744\n",
      "getting tweets before 768050432287526911\n",
      "getting tweets before 715922795235045376\n",
      "getting tweets before 672119967286865919\n",
      "getting tweets before 626459796066172927\n",
      "getting tweets before 585813623391457279\n",
      "getting tweets before 534361069519904768\n",
      "getting tweets before 520608291919892479\n",
      "getting tweets before 504985749880442879\n",
      "getting tweets before 487231001178165247\n",
      "getting tweets before 476731269859278847\n",
      "getting tweets before 460818477113417727\n",
      "getting tweets before 445935938037092351\n",
      "getting tweets before 444165022013730815\n",
      "...3215 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "#usernames = [\"StockTwits\", \"YahooFinance\", \"BobPisani\"] \n",
    "#usernames = [\"Benzinga\",\"jimcramer\",\"stockwits\"] \n",
    "usernames=[\"BobPisani\"]\n",
    "#Benzinga\n",
    "        \n",
    "def give_emoji_free_text(text):\n",
    "    allchars = [str for str in text.decode('utf-8')]\n",
    "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = ' '.join([str for str in text.decode('utf-8').split() if not any(i in str for i in emoji_list)])\n",
    "    return clean_text\n",
    "\n",
    "def get_all_tweets(screen_name):\n",
    "    #Twitter only allows access to a users most recent 3240 tweets with   this method\n",
    "\n",
    "    #authorize twitter, initialize tweepy\n",
    "    #auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    #auth.set_access_token(access_token, access_token_secret)\n",
    "    #api = tweepy.API(auth)\n",
    "\n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []  \n",
    "    print(\"screen name\",screen_name)\n",
    "    #make initial request for most recent tweets (200 is the maximum   allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
    "    alltweets.extend(new_tweets)\n",
    "    oldest = alltweets[-1].id - 1\n",
    "    \n",
    "    count=0\n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        print(\"getting tweets before %s\" % (oldest))\n",
    "\n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
    "\n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "        count+=1\n",
    "    #new_tweets = api.user_timeline(screen_name = screen_name,count=30000)\n",
    "\n",
    "    #save most recent tweets\n",
    "    #alltweets.extend(new_tweets)\n",
    "\n",
    "    #save the id of the oldest tweet less one\n",
    "   \n",
    "    print(\"...%s tweets downloaded so far\" % (len(alltweets)))\n",
    "        \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.remove('up') \n",
    "    stop_words.remove('down')\n",
    "    stop_words.remove('out')\n",
    "    stop_words.remove('off')\n",
    "    stop_words.remove('under')\n",
    "    stop_words.remove('over')\n",
    "    \n",
    "    #transform the tweepy tweets into a 2D array that will populate the csv \n",
    "    #outtweets = [[tweet.id_str, tweet.created_at, \n",
    "                  #give_emoji_free_text(str.split(str(tweet.text.encode(\"utf-8\")),'https')[0])] for tweet in alltweets]\n",
    "    #outtweets = [[tweet.id_str, tweet.created_at, \n",
    "                  #str.split(give_emoji_free_text(tweet.text.encode(\"utf-8\")),'https')[0]] for tweet in alltweets]\n",
    "    outtweets = [[tweet.created_at,\n",
    "                str.split(give_emoji_free_text(tweet.text.encode(\"utf-8\")),'https')[0]] for tweet in alltweets]\n",
    "    filteredlist=[]\n",
    "    for notweet in range(len(outtweets)):\n",
    "        word_tokens = word_tokenize(str(outtweets[notweet][1]))\n",
    "        filtered_sentence = [w for w in word_tokens if not w.upper() in [item.upper() for item in stop_words]]\n",
    "        #strf='\"' +' '.join(filtered_sentence)+ '\"'\n",
    "        strf=' '.join(filtered_sentence)\n",
    "        filteredlist.append(strf)\n",
    "        \n",
    "    createddates=[]\n",
    "    tweets=[]\n",
    "    \n",
    "    for flist in range(len(filteredlist)):\n",
    "        outtweets[flist][1]=filteredlist[flist]\n",
    "    totaltweets={}\n",
    "    for flis in range(len(outtweets)):\n",
    "        createddates.append(outtweets[flis][0])\n",
    "        tweets.append(outtweets[flis][1])\n",
    "    totaltweets[\"created_date\"]=createddates\n",
    "    totaltweets[\"tweets\"]=tweets\n",
    "    df = pd.DataFrame(totaltweets)\n",
    "    \n",
    "    calculatePolarities(df,screen_name)\n",
    "    #write the csv  \n",
    "    '''with open('%s_tweets.csv' % screen_name, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"created_at\",\"text\"])\n",
    "        writer.writerows(outtweets)'''\n",
    "\n",
    "    #pass\n",
    "def calculatePolarities(df,screen_name):\n",
    "    dfcp=df.copy()\n",
    "    \n",
    "    \n",
    "    tweetset=df[\"tweets\"]\n",
    "    polarit=[]\n",
    "    for tweet in tweetset:\n",
    "        analysis=TextBlob(tweet)\n",
    "        polarityval=analysis.sentiment.polarity\n",
    "        #print(\"plo%d\",polarityval)\n",
    "        \n",
    "        if(polarityval==0):\n",
    "            polarit.append(\"Neutral\")\n",
    "        elif(polarityval<0):\n",
    "            \n",
    "            polarit.append(\"Negative\")\n",
    "        elif(polarityval>0):\n",
    "            \n",
    "            polarit.append(\"Positive\")\n",
    "    \n",
    "    dfcp[\"sentiment\"]=polarit\n",
    "    dfcp.to_csv('{0}_tweets.csv'.format(screen_name))\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    #pass in the username of the account you want to download\n",
    "    for x in usernames:\n",
    "        get_all_tweets(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' oldest = alltweets[-1].id - 1\n",
    "    count=0\n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        print(\"getting tweets before %s\" % (oldest))\n",
    "\n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=3000,max_id=oldest)\n",
    "\n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "        count+=1'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
